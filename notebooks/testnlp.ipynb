{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gcloud' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth activate-service-account --key-file=noted-branch-140913-9bde9d49c946.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-language\n",
      "  Downloading google_cloud_language-2.16.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language)\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 (from google-cloud-language)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-language)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-cloud-language) (5.29.2)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (1.68.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language)\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-language)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-language)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-language)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language)\n",
      "  Downloading grpcio-1.69.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-language)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-language) (2024.12.14)\n",
      "Downloading google_cloud_language-2.16.0-py2.py3-none-any.whl (163 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading grpcio_status-1.69.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.69.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.4 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.3/4.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.1/4.4 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, proto-plus, grpcio, googleapis-common-protos, cachetools, rsa, pyasn1-modules, grpcio-status, google-auth, google-api-core, google-cloud-language\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.1\n",
      "    Uninstalling grpcio-1.68.1:\n",
      "      Successfully uninstalled grpcio-1.68.1\n",
      "Successfully installed cachetools-5.5.0 google-api-core-2.24.0 google-auth-2.37.0 google-cloud-language-2.16.0 googleapis-common-protos-1.66.0 grpcio-1.69.0 grpcio-status-1.69.0 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-language\n",
    "%pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/user/Developer/Code/AI/nlp_google/noted-branch-140913-9bde9d49c946.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/user/Developer/Code/AI/nlp_google/cv.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Contoh penggunaan\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/user/Developer/Code/AI/nlp_google/cv.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Simpan hasil ke file teks\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mpdf_to_text\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf_to_text\u001b[39m(pdf_path):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Membuka file PDF\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(file)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Inisialisasi variabel untuk menyimpan teks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/user/Developer/Code/AI/nlp_google/cv.pdf'"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    # Membuka file PDF\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Inisialisasi variabel untuk menyimpan teks\n",
    "        text = \"\"\n",
    "        \n",
    "        # Loop melalui setiap halaman dan ekstrak teks\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()  # Ekstrak teks dari halaman\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Contoh penggunaan\n",
    "pdf_path = \"/Users/user/Developer/Code/AI/nlp_google/cv.pdf\"\n",
    "text = pdf_to_text(pdf_path)\n",
    "\n",
    "# Simpan hasil ke file teks\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(text)\n",
    "\n",
    "print(\"Konversi selesai!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Sentiment: -0.8999999761581421\n",
      "Magnitudo Sentiment: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    document = language_v1.Document(\n",
    "        content=text,\n",
    "        type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "    )\n",
    "\n",
    "    response = client.analyze_sentiment(request={'document': document})\n",
    "    sentiment = response.document_sentiment\n",
    "\n",
    "    print(f\"Score Sentiment: {sentiment.score}\")\n",
    "    print(f\"Magnitudo Sentiment: {sentiment.magnitude}\")\n",
    "\n",
    "text = \"I hate you !!!\"\n",
    "analyze_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:\n",
      "full_name: Sarman Chisara\n",
      "nidn: 1989071120240610\n",
      "education: S1 S2\n",
      "work_history: Job Job Position Position\n",
      "research_history: Project Research Research\n",
      "skills: CodeIgniter AI\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import language_v1\n",
    "import re\n",
    "\n",
    "# Function to analyze entities\n",
    "def analyze_entities(text):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    document = language_v1.Document(\n",
    "        content=text,\n",
    "        type_=language_v1.Document.Type.PLAIN_TEXT,\n",
    "        language=\"en\"  # Setting the language to English for entity extraction\n",
    "    )\n",
    "\n",
    "    response = client.analyze_entities(request={'document': document})\n",
    "    entities = {}\n",
    "\n",
    "    # Process the entities\n",
    "    for entity in response.entities:\n",
    "        # If the entity is a person, we'll store it as a full name\n",
    "        if language_v1.Entity.Type(entity.type_).name == 'PERSON':\n",
    "            entities['full_name'] = entity.name\n",
    "\n",
    "    # Manually extract additional entities using regular expressions and keywords\n",
    "\n",
    "    # Extract NIDN (16-digit ID)\n",
    "    nidn_match = re.search(r'\\d{16}', text)  # NIDN usually has 16 digits\n",
    "    if nidn_match:\n",
    "        entities['nidn'] = nidn_match.group()\n",
    "\n",
    "    # Extract Education (S1, S2, or degree related terms)\n",
    "    education_keywords = [\"S1\", \"S2\", \"Bachelor\", \"Master\", \"Doctor\"]\n",
    "    education = [word for word in text.split() if word in education_keywords]\n",
    "    if education:\n",
    "        entities['education'] = \" \".join(education)\n",
    "\n",
    "    # Extract Work History (Position, Job)\n",
    "    work_keywords = [\"position\", \"job\", \"Lecturer\", \"Assistant Expert\", \"IT Staff\"]\n",
    "    work_history = [word for word in text.split() if word.lower() in work_keywords]\n",
    "    if work_history:\n",
    "        entities['work_history'] = \" \".join(work_history)\n",
    "\n",
    "    # Extract Research History (Research, Thesis, Project)\n",
    "    research_keywords = [\"research\", \"study\", \"project\", \"Thesis\", \"researcher\"]\n",
    "    research_history = [word for word in text.split() if word.lower() in research_keywords]\n",
    "    if research_history:\n",
    "        entities['research_history'] = \" \".join(research_history)\n",
    "\n",
    "    # Extract Skills (Programming languages, frameworks, etc.)\n",
    "    skills_keywords = [\"Python\", \"Java\", \"Machine Learning\", \"AI\", \"CodeIgniter\", \"Deep Learning\"]\n",
    "    skills = [word for word in text.split() if word in skills_keywords]\n",
    "    if skills:\n",
    "        entities['skills'] = \" \".join(skills)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Function to read and analyze file content\n",
    "def read_and_analyze_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    entities = analyze_entities(text)\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/user/Developer/Code/AI/nlp_google/output.txt'  # Path to your converted text file\n",
    "entities = read_and_analyze_file(file_path)\n",
    "\n",
    "print(\"Analysis Results:\")\n",
    "for key, value in entities.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.1-cp39-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.1-cp39-abi3-macosx_10_9_x86_64.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:\n",
      "full_name: Tim Sumber Anggaran\n",
      "nidn: 1989071120240610\n",
      "education: S1 S1 S2\n",
      "work_history: Position\n",
      "skills: AI\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from google.cloud import language_v1\n",
    "import re\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Function to analyze entities\n",
    "def analyze_entities(text):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    document = language_v1.Document(\n",
    "        content=text,\n",
    "        type_=language_v1.Document.Type.PLAIN_TEXT,\n",
    "        language=\"en\"  # Setting the language to English for entity extraction\n",
    "    )\n",
    "\n",
    "    response = client.analyze_entities(request={'document': document})\n",
    "    entities = {}\n",
    "\n",
    "    # Process the entities\n",
    "    for entity in response.entities:\n",
    "        # If the entity is a person, we'll store it as a full name\n",
    "        if language_v1.Entity.Type(entity.type_).name == 'PERSON':\n",
    "            entities['full_name'] = entity.name\n",
    "\n",
    "    # Manually extract additional entities using regular expressions and keywords\n",
    "\n",
    "    # Extract NIDN (16-digit ID)\n",
    "    nidn_match = re.search(r'\\d{16}', text)  # NIDN usually has 16 digits\n",
    "    if nidn_match:\n",
    "        entities['nidn'] = nidn_match.group()\n",
    "\n",
    "    # Extract Education (S1, S2, or degree related terms)\n",
    "    education_keywords = [\"S1\", \"S2\", \"Bachelor\", \"Master\", \"Doctor\"]\n",
    "    education = [word for word in text.split() if word in education_keywords]\n",
    "    if education:\n",
    "        entities['education'] = \" \".join(education)\n",
    "\n",
    "    # Extract Work History (Position, Job)\n",
    "    work_keywords = [\"position\", \"job\", \"Lecturer\", \"Assistant Expert\", \"IT Staff\"]\n",
    "    work_history = [word for word in text.split() if word.lower() in work_keywords]\n",
    "    if work_history:\n",
    "        entities['work_history'] = \" \".join(work_history)\n",
    "\n",
    "    # Extract Research History (Research, Thesis, Project)\n",
    "    research_keywords = [\"research\", \"study\", \"project\", \"Thesis\", \"researcher\"]\n",
    "    research_history = [word for word in text.split() if word.lower() in research_keywords]\n",
    "    if research_history:\n",
    "        entities['research_history'] = \" \".join(research_history)\n",
    "\n",
    "    # Extract Skills (Programming languages, frameworks, etc.)\n",
    "    skills_keywords = [\"Python\", \"Java\", \"Machine Learning\", \"AI\", \"CodeIgniter\", \"Deep Learning\"]\n",
    "    skills = [word for word in text.split() if word in skills_keywords]\n",
    "    if skills:\n",
    "        entities['skills'] = \" \".join(skills)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Function to read and analyze a PDF file\n",
    "def analyze_pdf_file(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    entities = analyze_entities(text)\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "pdf_path = '/Users/user/Developer/Code/AI/nlp_google/cv.pdf'  # Path to your PDF file\n",
    "entities = analyze_pdf_file(pdf_path)\n",
    "\n",
    "print(\"Analysis Results:\")\n",
    "for key, value in entities.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Analisis Entitas:\n",
      "full_name: Lecturer\n",
      "education: Bachelor\n",
      "skills: Python\n"
     ]
    }
   ],
   "source": [
    "# Contoh sederhana untuk analisis entitas dari teks menggunakan Google NLP API\n",
    "simple_text = \"John Doe has a Bachelor degree in Computer Science and works as a Lecturer. He has skills in Python and AI.\"\n",
    "\n",
    "# Menggunakan fungsi analyze_entities untuk menganalisis teks sederhana\n",
    "simple_entities = analyze_entities(simple_text)\n",
    "\n",
    "print(\"Hasil Analisis Entitas:\")\n",
    "for key, value in simple_entities.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Entitas: Google\n",
      "Tipe Entitas: ORGANIZATION\n",
      "Salience: 0.8855599761009216\n",
      "Nama Entitas: California\n",
      "Tipe Entitas: LOCATION\n",
      "Salience: 0.07895361632108688\n",
      "Nama Entitas: Mountain View\n",
      "Tipe Entitas: LOCATION\n",
      "Salience: 0.035486411303281784\n"
     ]
    }
   ],
   "source": [
    "def analyze_entities(text):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    document = language_v1.Document(\n",
    "        content=text,\n",
    "        type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "    )\n",
    "\n",
    "    response = client.analyze_entities(request={'document': document})\n",
    "    for entity in response.entities:\n",
    "        print(f\"Nama Entitas: {entity.name}\")\n",
    "        print(f\"Tipe Entitas: {language_v1.Entity.Type(entity.type_).name}\")\n",
    "        print(f\"Salience: {entity.salience}\")\n",
    "\n",
    "text = \"Google is a leading tech company based in Mountain View, California.\"\n",
    "analyze_entities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
